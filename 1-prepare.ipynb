{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4: Topic modeling\n",
    "\n",
    "Use topic models to explore hotel reviews\n",
    "\n",
    "Objectives:\n",
    "* tokenize with MWEs using spacy\n",
    "* estimate LDA topic models with tomotopy\n",
    "* visualize and evaluate topic models\n",
    "* apply topic models to interpretation of hotel reviews\n",
    "\n",
    "## Prepare data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Prepare reviews\n",
    "\n",
    "This section is that same as what we did back in Lab 1: \n",
    "\n",
    "* Load the JSON file for the hotel reviews\n",
    "* Guess the language of each review\n",
    "* Select only the ones that are in English\n",
    "* Add rating categories (Overall, etc) as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycld2\n",
    "\n",
    "df = pd.read_json('s3://ling583/review.json.gz', lines=True,\n",
    "                  storage_options={'anon': True})\n",
    "\n",
    "\n",
    "def guess_lang(text):\n",
    "    try:\n",
    "        reliable, _, langs = pycld2.detect(\n",
    "            text, isPlainText=True, hintLanguage='en')\n",
    "        if reliable:\n",
    "            return langs[0][0]\n",
    "    except pycld2.error as e:\n",
    "        pass\n",
    "    return np.NaN\n",
    "\n",
    "\n",
    "df['lang'] = df['text'].progress_apply(guess_lang)\n",
    "df = df[df['lang'] == 'ENGLISH'].reset_index(drop=True)\n",
    "df = pd.concat([df, pd.json_normalize(df['ratings'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll get rid of columns that we're not going to need. That makes the tables easier to read and saves a little bit of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ratings', 'author', 'num_helpful_votes', 'via_mobile', 'lang', 'id', 'check_in_front_desk',\n",
    "              'business_service_(e_g_internet_access)'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Add hotel info\n",
    "\n",
    "The data we looked at in the first lab included the text of the reviews but almost nothing about what hotels they were reviews of. In the data in `review.json.gz`, the column `offering_id` is a code number identifying the hotel indexed to data in `offering.json.gz`. So first we'll load that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offering = pd.read_json('s3://ling583/offering.json.gz',\n",
    "                        lines=True, storage_options={'anon': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And move some address info into it's own columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offering = pd.concat([offering, pd.json_normalize(offering['address'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll combine the review info from `df` with the corresponding hotel info from `offering` (in SQL terms, this is an [inner join](https://en.wikipedia.org/wiki/Join_(SQL)#Inner_join) operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(offering[['locality', 'id', 'name']],\n",
    "              left_on='offering_id', right_on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns we don't need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id', 'offering_id'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the result to a local datafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('hotels.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Find domain-specific terms\n",
    "\n",
    "The next thing we need to do is find domain-specific terminology (multiword expressions, or MWEs) that is relevant for hotel reviews (like \"front desk\" and \"room key\").  We'll follow the same procedure as we did in Lab 3 with one different: the tag sequence we're using for matching allows both common nouns (e.g., \"coffee pot\") and proper nouns (e.g., \"New York\"). Multi-word proper names aren't usually technical terms in scientific literature, but they are important for hotel reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', exclude=[\n",
    "                 'parser', 'ner', 'lemmatizer', 'attribute_ruler'])\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Term', [[{'TAG': {'IN': ['JJ', 'NN', 'NNP']}},\n",
    "                      {'TAG': {'IN': ['JJ', 'NN', 'IN',\n",
    "                                      'HYPH', 'NNP']}, 'OP': '*'},\n",
    "                      {'TAG': {'IN': ['NN', 'NNP']}}]])\n",
    "\n",
    "\n",
    "def get_candidates(text):\n",
    "    doc = nlp(text)\n",
    "    spans = matcher(doc, as_spans=True)\n",
    "    return [tuple(tok.norm_ for tok in span) for span in spans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of reviews to get through, so we'll parallelize things using dask. In order to run this notebook you'll need to **start a dask cluster** and **replace the port number of the client** (xxxxx below) with the right value for your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:xxxxx\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save processing time, we'll just use a sub-sample of 100,000 reviews to find MWEs. (Feel free to increase that if you want, but you'll need to adjust `theta` below accordingly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "\n",
    "texts = dd.from_pandas(df['text'].sample(\n",
    "    100000, random_state=19), npartitions=50).to_bag()\n",
    "\n",
    "graph = texts.map(get_candidates).flatten().frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "candidates = graph.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "def get_subterms(term):\n",
    "    k = len(term)\n",
    "    for m in range(k-1, 1, -1):\n",
    "        yield from ngrams(term, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import log2\n",
    "\n",
    "freqs = defaultdict(Counter)\n",
    "for c, f in candidates:\n",
    "    freqs[len(c)][c] += f\n",
    "\n",
    "\n",
    "def c_value(F, theta):\n",
    "\n",
    "    termhood = Counter()\n",
    "    longer = defaultdict(list)\n",
    "\n",
    "    for k in sorted(F, reverse=True):\n",
    "        for term in F[k]:\n",
    "            if term in longer:\n",
    "                discount = sum(longer[term]) / len(longer[term])\n",
    "            else:\n",
    "                discount = 0\n",
    "            c = log2(k) * (F[k][term] - discount)\n",
    "            if c > theta:\n",
    "                termhood[term] = c\n",
    "                for subterm in get_subterms(term):\n",
    "                    if subterm in F[len(subterm)]:\n",
    "                        longer[subterm].append(F[k][term])\n",
    "    return termhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = c_value(freqs, theta=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, c in terms.most_common(20):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:5d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, c in tail(20, terms.most_common()):\n",
    "    print(f'{c:8.2f} {freqs[len(t)][t]:5d} {\" \".join(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last of all, we'll save the MWEs that we've found for use in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hotel-terms.txt', 'w') as f:\n",
    "    for t in terms:\n",
    "        print(' '.join(t), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
